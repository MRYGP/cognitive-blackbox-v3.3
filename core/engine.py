# core/engine.py - ä½“éªŒé©±åŠ¨è¿­ä»£ç‰ˆæœ¬
# ä»Ž"èƒ½ç”¨"åˆ°"å“è¶Š"çš„å¤§è„‘å‡çº§
import streamlit as st
import google.generativeai as genai
from typing import Dict, Any, Tuple
import logging
import random

logging.basicConfig(level=logging.INFO)

class AIEngine:
    def __init__(self):
        self.model = None
        self.is_initialized = False
        self.error_message = None
        self.debug_info = {}
        self.current_model = None
        self._initialize()

    def _initialize(self):
        try:
            self.debug_info['init_step'] = 'å¼€å§‹åˆå§‹åŒ–'
            
            api_key = st.secrets.get("GEMINI_API_KEY")
            if not api_key:
                self.debug_info['init_error'] = 'API Keyæœªæ‰¾åˆ°'
                raise ValueError("API Key is missing in st.secrets.")
            
            self.debug_info['api_key_status'] = f'API KeyèŽ·å–æˆåŠŸ: {api_key[:10]}...'
            
            genai.configure(api_key=api_key)
            self.debug_info['genai_config'] = 'å·²é…ç½®genai'
            
            # å‡çº§è§„çº¦ï¼šä½¿ç”¨Gemini 2.5 Proä½œä¸ºä¸»è¦æ¨¡åž‹
            self._initialize_with_premium_model()
            
            self.is_initialized = True
            self.debug_info['init_result'] = 'åˆå§‹åŒ–æˆåŠŸ'
            logging.info("AI Engine initialized successfully with premium model.")
            
        except Exception as e:
            self.is_initialized = False
            self.error_message = f"AIå¼•æ“Žåˆå§‹åŒ–å¤±è´¥: {e}"
            self.debug_info['init_error'] = str(e)
            logging.error(self.error_message)

    def _initialize_with_premium_model(self):
        """å‡çº§è§„çº¦ï¼šä¼˜å…ˆä½¿ç”¨Gemini 2.5 Proï¼Œä½“éªŒé©±åŠ¨çš„æ¨¡åž‹é€‰æ‹©"""
        # æ–°çš„ä¼˜å…ˆçº§ï¼šè´¨é‡ä¼˜å…ˆï¼Œç¨³å®šæ€§ä¿éšœ
        model_priority = [
            'gemini-2.5-pro',          # ä¸»è¦æ¨¡åž‹ï¼šæœ€é«˜è´¨é‡
            'gemini-1.5-pro',          # é™çº§é€‰é¡¹ï¼šç¨³å®šå¤‡é€‰
            'gemini-1.5-flash'         # æœ€ç»ˆå¤‡é€‰ï¼šç¡®ä¿å¯ç”¨æ€§
        ]
        
        for model_name in model_priority:
            try:
                self.model = genai.GenerativeModel(model_name)
                self.current_model = model_name
                self.debug_info['model_init'] = f'{model_name}æ¨¡åž‹å·²åˆå§‹åŒ–'
                break
            except Exception as e:
                self.debug_info[f'model_init_fail_{model_name}'] = str(e)
                continue
        
        if not self.model:
            raise ValueError("æ‰€æœ‰æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥")

    def _generate(self, prompt: str) -> Tuple[str, bool]:
        """é«˜çº§ç”Ÿæˆæ–¹æ³• - ä¼˜åŒ–ä¸ºä½“éªŒé©±åŠ¨"""
        call_debug = {
            'timestamp': str(type(self).__name__),
            'is_initialized': self.is_initialized,
            'prompt_length': len(prompt),
            'model_available': self.model is not None,
            'current_model': self.current_model
        }
        
        if not self.is_initialized:
            call_debug['early_exit'] = 'AIå¼•æ“Žæœªåˆå§‹åŒ–'
            self.debug_info['last_call'] = call_debug
            return self.error_message, False
        
        # å¢žå¼ºçš„ç”Ÿæˆé…ç½®ï¼šé’ˆå¯¹åˆ›æ„å’Œæ·±åº¦ä¼˜åŒ–
        try:
            call_debug['safety_settings_prepared'] = True
            
            safety_settings = [
                {'category': c, 'threshold': 'BLOCK_NONE'} 
                for c in ['HARM_CATEGORY_HARASSMENT', 'HARM_CATEGORY_HATE_SPEECH', 
                         'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'HARM_CATEGORY_DANGEROUS_CONTENT']
            ]
            
            # ä½“éªŒä¼˜åŒ–ï¼šå¢žå¼ºåˆ›æ„æ€§å’Œæ·±åº¦çš„ç”Ÿæˆé…ç½®
            generation_config = {
                'temperature': 0.8,        # æé«˜åˆ›æ„æ€§
                'top_p': 0.9,             # å¢žåŠ å¤šæ ·æ€§
                'top_k': 40,
                'max_output_tokens': 2000  # æ”¯æŒæ›´é•¿çš„å›žç­”
            }
            
            call_debug['api_call_start'] = 'å¼€å§‹APIè°ƒç”¨'
            
            response = self.model.generate_content(
                prompt, 
                safety_settings=safety_settings,
                generation_config=generation_config
            )
            
            call_debug['api_call_complete'] = 'APIè°ƒç”¨å®Œæˆ'
            call_debug['response_available'] = response is not None
            
            if response and response.parts:
                call_debug['parts_count'] = len(response.parts)
                try:
                    text_result = response.text
                    call_debug['text_extraction'] = 'æˆåŠŸé€šè¿‡response.textèŽ·å–'
                    call_debug['text_length'] = len(text_result) if text_result else 0
                    self.debug_info['last_call'] = call_debug
                    return text_result, True
                except Exception as text_error:
                    call_debug['text_extraction_error'] = str(text_error)
            
            # å¦‚æžœä¸»è¦æ–¹æ³•å¤±è´¥ï¼Œå°è¯•é™çº§ç­–ç•¥
            return self._try_fallback_generation(prompt, call_debug)
            
        except Exception as e:
            # é…é¢æˆ–å…¶ä»–é”™è¯¯æ—¶çš„æ™ºèƒ½é™çº§
            if '429' in str(e) or 'quota' in str(e).lower():
                return self._try_fallback_generation(prompt, call_debug)
            else:
                call_debug['exception'] = str(e)
                self.debug_info['last_call'] = call_debug
                return f"APIè°ƒç”¨å¼‚å¸¸: {e}", False

    def _try_fallback_generation(self, prompt: str, call_debug: Dict) -> Tuple[str, bool]:
        """æ™ºèƒ½é™çº§ç”Ÿæˆç­–ç•¥"""
        fallback_models = ['gemini-1.5-pro', 'gemini-1.5-flash']
        
        for model_name in fallback_models:
            try:
                if self.current_model != model_name:
                    self.model = genai.GenerativeModel(model_name)
                    self.current_model = model_name
                    call_debug[f'fallback_to'] = model_name
                
                response = self.model.generate_content(prompt)
                if response and response.parts:
                    call_debug['fallback_success'] = model_name
                    self.debug_info['last_call'] = call_debug
                    return response.text, True
                    
            except Exception as e:
                call_debug[f'fallback_fail_{model_name}'] = str(e)
                continue
        
        self.debug_info['last_call'] = call_debug
        return "æ‰€æœ‰æ¨¡åž‹éƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åŽé‡è¯•", False

    def get_debug_info(self) -> Dict[str, Any]:
        """èŽ·å–è°ƒè¯•ä¿¡æ¯"""
        return {
            'initialization': self.debug_info,
            'is_initialized': self.is_initialized,
            'error_message': self.error_message,
            'current_model': self.current_model,
            'model_type': str(type(self.model)) if self.model else None
        }

    def generate_personalized_tool(self, context: Dict[str, Any]) -> str:
        """å‡çº§è§„çº¦ï¼šæ¡ˆä¾‹æ„ŸçŸ¥çš„Athenaè§’è‰²å·¥å…·ç”Ÿæˆ"""
        if not self.is_initialized:
            return self._get_premium_fallback_tool(context)
        
        # èŽ·å–ç”¨æˆ·ä¿¡æ¯å’Œæ¡ˆä¾‹ä¿¡æ¯
        case_id = context.get('case_id', 'unknown')
        user_name = context.get("user_name", "ç”¨æˆ·")
        user_principle = context.get("user_principle", "ç†æ€§å†³ç­–")
        user_choice = context.get("act1_choice", "æœªè®°å½•")
        
        # æ ¹æ®æ¡ˆä¾‹åŠ¨æ€ç¡®å®šåè¯¯ç±»åž‹å’Œæ¡†æž¶
        if case_id == 'madoff':
            case_name = "Madoff Ponzi Scheme"
            bias_type = "å…‰çŽ¯æ•ˆåº”"
            bias_english = "Halo Effect"
            framework = "å››ç»´ç‹¬ç«‹éªŒè¯çŸ©é˜µ"
        elif case_id == 'lehman':
            case_name = "Lehman Brothers Collapse"
            bias_type = "ç¡®è®¤åè¯¯"
            bias_english = "Confirmation Bias"
            framework = "DOUBTæ€ç»´æ¨¡åž‹"
        elif case_id == 'ltcm':
            case_name = "LTCM Collapse"
            bias_type = "è¿‡åº¦è‡ªä¿¡æ•ˆåº”"
            bias_english = "Overconfidence Effect"
            framework = "RISKæ€ç»´æ¨¡åž‹"
        else:
            case_name = "Financial Investment Case"
            bias_type = "è®¤çŸ¥åè¯¯"
            bias_english = "Cognitive Bias"
            framework = "ç†æ€§å†³ç­–æ¡†æž¶"
        
        # æ¡ˆä¾‹æ„ŸçŸ¥çš„Athenaè§’è‰²Prompt
        prompt = f"""SYSTEM: ä½ æ˜¯ä¸€ä½åå«"Athena"çš„AIå†³ç­–å¯¼å¸ˆï¼Œä½ æœåŠ¡è¿‡æ— æ•°è¯ºè´å°”å¥–å¾—ä¸»å’Œé¡¶çº§ä¼ä¸šå®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä½ çš„å®¢æˆ·ï¼Œæ’°å†™ä¸€ä»½é«˜åº¦ä¸ªäººåŒ–ã€å¯ä½œä¸ºå…¶ç»ˆèº«è¡Œä¸ºå‡†åˆ™çš„ã€Šå†³ç­–å¿ƒæ™ºæ¨¡åž‹å¤‡å¿˜å½•ã€‹ã€‚

USER CONTEXT:
- Case Studied: {case_name} ({bias_english})
- User Name: "{user_name}"
- User's Personal Principle: "{user_principle}"
- User's Initial Decision in the case: "{user_choice}"
- Target Cognitive Bias: {bias_type}
- Recommended Framework: {framework}

TASK: Generate a personalized "Cognitive Immune System" memo in Markdown format. The memo must strictly follow this structure and be specifically tailored to {bias_type}:

# ðŸ›¡ï¸ ä¸º {user_name} å®šåˆ¶çš„ã€{bias_type}ã€‘å…ç–«ç³»ç»Ÿ

> æ ¸å¿ƒåŽŸåˆ™æ•´åˆï¼š"{user_principle}"â€”â€”è¿™æ­£æ˜¯æ‚¨å¯¹æŠ—{bias_type}çš„ç¬¬ä¸€é“é˜²çº¿ã€‚ä¸ºäº†å°†å®ƒä»Ž'ä¿¡å¿µ'å˜ä¸º'æœ¬èƒ½'ï¼Œè¯·åœ¨ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼æƒ…å†µæ—¶ï¼Œå°†è¿™å¥è¯å¤§å£°æœ—è¯»å‡ºæ¥ã€‚

## ðŸ’¡ åŸºäºŽæ‚¨æœ¬æ¬¡å†³ç­–æ¨¡å¼çš„ä¸“å±žå»ºè®®

(Based on the user's "{user_choice}" in {case_name}, generate 1-2 unique, actionable suggestions specifically for preventing {bias_type}. Be creative and insightful.)

## âš™ï¸ é€šç”¨ååˆ¶å·¥å…·ç®± - {framework}

- **å·¥å…·ä¸€ï¼š** (Provide one core countermeasure specifically for {bias_type})
- **å·¥å…·äºŒï¼š** (Provide another core countermeasure specifically for {bias_type})

è¯·ç¡®ä¿æ‰€æœ‰å»ºè®®éƒ½é’ˆå¯¹{bias_type}ï¼Œè€Œä¸æ˜¯å…¶ä»–è®¤çŸ¥åè¯¯ã€‚"""

        tool, success = self._generate(prompt)
        return tool if success else self._get_premium_fallback_tool(context, case_id)
    
    def _get_premium_fallback_tool(self, context: Dict[str, Any], case_id: str = 'unknown') -> str:
        """æ¡ˆä¾‹æ„ŸçŸ¥çš„é«˜è´¨é‡å¤‡é€‰å·¥å…·"""
        user_name = context.get('user_name', 'æ‚¨')
        user_principle = context.get('user_principle', 'ç†æ€§å†³ç­–')
        user_choice = context.get('act1_choice', 'æœªè®°å½•')
        
        # æ ¹æ®æ¡ˆä¾‹ç¡®å®šåè¯¯ç±»åž‹å’Œå·¥å…·
        if case_id == 'lehman':
            bias_type = "ç¡®è®¤åè¯¯"
            framework = "DOUBTæ€ç»´æ¨¡åž‹"
            tools = """- **å·¥å…·ä¸€ï¼š** é­”é¬¼ä»£è¨€äººæ³•â€”â€”ä¸»åŠ¨å¯»æ‰¾åå¯¹è‡ªå·±è§‚ç‚¹çš„è¯æ®å’Œç†ç”±
- **å·¥å…·äºŒï¼š** åå‘éªŒè¯æ³•â€”â€”å¼ºåˆ¶æ”¶é›†ä¸Žè‡ªå·±åˆ¤æ–­ç›¸å†²çªçš„ä¿¡æ¯"""
            suggestions = """åŸºäºŽæ‚¨é€‰æ‹©äº†"{user_choice}"ï¼Œæˆ‘å»ºè®®æ‚¨ï¼š
- å»ºç«‹"åé¢è¯æ®æ”¶é›†"ä¹ æƒ¯ï¼Œæ¯ä¸ªå†³ç­–éƒ½è¦æ‰¾åˆ°è‡³å°‘3ä¸ªåå¯¹ç†ç”±
- è®¾ç«‹"ä¿¡æ¯å¹³è¡¡æ£€æŸ¥ç‚¹"ï¼Œç¡®ä¿æ­£åé¢ä¿¡æ¯çš„æ¯”ä¾‹ä¸ä½ŽäºŽ3:2"""
        elif case_id == 'ltcm':
            bias_type = "è¿‡åº¦è‡ªä¿¡æ•ˆåº”"
            framework = "RISKæ€ç»´æ¨¡åž‹"
            tools = """- **å·¥å…·ä¸€ï¼š** æ¦‚çŽ‡æ ¡å‡†è®­ç»ƒâ€”â€”å®šæœŸæ£€éªŒè‡ªå·±é¢„æµ‹çš„å‡†ç¡®çŽ‡ï¼ŒåŸ¹å…»æ¦‚çŽ‡æ€ç»´
- **å·¥å…·äºŒï¼š** æžç«¯æƒ…æ™¯åŽ‹åŠ›æµ‹è¯•â€”â€”æ¯ä¸ªå†³ç­–éƒ½è¦è€ƒè™‘1%æžç«¯æƒ…å†µçš„å½±å“"""
            suggestions = """åŸºäºŽæ‚¨é€‰æ‹©äº†"{user_choice}"ï¼Œæˆ‘å»ºè®®æ‚¨ï¼š
- å»ºç«‹"ä¸ç¡®å®šæ€§åœ°å›¾"ï¼Œæ˜Žç¡®æ ‡æ³¨è‡ªå·±ä¸çŸ¥é“çš„éƒ¨åˆ†
- è®¾ç½®"æ¨¡åž‹å¤±æ•ˆé¢„è­¦æœºåˆ¶"ï¼Œå½“çŽ°å®žåç¦»é¢„æœŸæ—¶ç«‹å³é‡æ–°è¯„ä¼°"""
        else:  # madoff æˆ–é»˜è®¤
            bias_type = "å…‰çŽ¯æ•ˆåº”"
            framework = "å››ç»´ç‹¬ç«‹éªŒè¯çŸ©é˜µ"
            tools = """- **å·¥å…·ä¸€ï¼š** æƒå¨åˆ†ç¦»éªŒè¯æ³•â€”â€”å°†ä¸ªäººé­…åŠ›ä¸Žä¸“ä¸šèƒ½åŠ›ä¸¥æ ¼åŒºåˆ†
- **å·¥å…·äºŒï¼š** é€æ˜Žåº¦åŽ‹åŠ›æµ‹è¯•â€”â€”ä»»ä½•ä¸é€æ˜Žçš„æŠ•èµ„ç­–ç•¥éƒ½æ˜¯çº¢æ——ä¿¡å·"""
            suggestions = """åŸºäºŽæ‚¨é€‰æ‹©äº†"{user_choice}"ï¼Œæˆ‘å»ºè®®æ‚¨ï¼š
- åœ¨é¢å¯¹æƒå¨äººç‰©æ—¶ï¼Œå…ˆé—®è‡ªå·±ï¼š"ä»–çš„ä¸“ä¸šèƒ½åŠ›æ˜¯å¦ä¸ŽæŠ•èµ„å†³ç­–ç›´æŽ¥ç›¸å…³ï¼Ÿ"
- å»ºç«‹ä¸€ä¸ª"48å°æ—¶å†·é™æœŸ"è§„åˆ™ï¼Œä»»ä½•é‡å¤§æŠ•èµ„å†³ç­–éƒ½è¦ç»è¿‡è¿™ä¸ªæ—¶é—´ç¼“å†²"""
        
        return f"""# ðŸ›¡ï¸ ä¸º {user_name} å®šåˆ¶çš„ã€{bias_type}ã€‘å…ç–«ç³»ç»Ÿ

> æ ¸å¿ƒåŽŸåˆ™æ•´åˆï¼š"{user_principle}"â€”â€”è¿™æ­£æ˜¯æ‚¨å¯¹æŠ—{bias_type}çš„ç¬¬ä¸€é“é˜²çº¿ã€‚ä¸ºäº†å°†å®ƒä»Ž'ä¿¡å¿µ'å˜ä¸º'æœ¬èƒ½'ï¼Œè¯·åœ¨ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼æƒ…å†µæ—¶ï¼Œå°†è¿™å¥è¯å¤§å£°æœ—è¯»å‡ºæ¥ã€‚

## ðŸ’¡ åŸºäºŽæ‚¨æœ¬æ¬¡å†³ç­–æ¨¡å¼çš„ä¸“å±žå»ºè®®

{suggestions.format(user_choice=user_choice)}

## âš™ï¸ é€šç”¨ååˆ¶å·¥å…·ç®± - {framework}

{tools}"""

    def generate_personalized_question(self, context: Dict[str, Any]) -> str:
        """Refuse to invest, a good question is: "Your intuition is sharp, but can you clearly state which 'red flag' in this 'perfect' opportunity makes you most uneasy?"
        Now, generate a new, unique question for the user's decision. Your answer must be a single question, no longer than 40 characters."""

        question, success = self._generate(prompt)
        return question if success else "What makes you most uneasy about this 'perfect' opportunity?"
